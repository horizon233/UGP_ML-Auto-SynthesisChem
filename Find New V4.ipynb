{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新实验数据生成-V4-LC\n",
    "#### 拉丁超立方采样-预测还原概率筛选-最远点采样筛选-按总体积换算\n",
    "目标是补充数据点，提高还原产物的概率\n",
    "最后更新：2022.04.05 戴以恒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import joblib\n",
    "c_time = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n",
    "c_time_m = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数\n",
    "# ======== System Setup ========\n",
    "DIR = 'New_Points V4'\n",
    "EPOCH = 100 # 每个c1,c2条件绘制1张相图需要EPOCH轮，注意合理设置这些参数，防止运行时间暴增\n",
    "\n",
    "C_LIMIT = [[0.05, 1.6], [0.05, 2.2], [3.0, 50.0]]    # 超立方体的边界，指定c1c2c3，c4c5由计算获得\n",
    "\n",
    "NUM_GENERATE = 150  # 预生成的数据点数量\n",
    "NUM_BATCH = 30  # 拉丁超立方采样的批次\n",
    "NUM_FINAL = 72  # 最终保留的样本数，这个数值增大，最终数据集内部的最近欧几里得距离会下降\n",
    "\n",
    "C5_PRODUCT_TIME = 1.986  # 2.601代表5:1加水，1.949代表5:0.75加水\n",
    "C0 = [1.99, 1.6553, 100.0, 60.0]     # 原始的溶液浓度，单位为M，即mmol/L    c1:3.56(5:1)；3.72(5:0.75)\n",
    "\n",
    "V_TOTAL = 1.2    # 总体积，mL\n",
    "\n",
    "RED_STANDARD = 0.5   # 还原置信概率\n",
    "YF_STANDARD = 0.4    # 黄色荧光置信概率\n",
    "# ======== Fit Data Input ========\n",
    "INPUT_X_C = 'Features_499_5_c.csv'\n",
    "INPUT_Y_C = 'Values_Red_499_R.csv'\n",
    "INPUT_TITLE = 'Title_5_c.csv'\n",
    "TRAIN_TEST_SPLIT_1 = 0.85\n",
    "INPUT_X_P = 'Features_68_5_c.csv'\n",
    "INPUT_Y_P = 'Values_YF_68.csv'\n",
    "TRAIN_TEST_SPLIT_2 = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "DIR += '_'+c_time\n",
    "os.mkdir(DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_C = np.loadtxt(INPUT_X_C, delimiter=',')\n",
    "title = np.loadtxt(INPUT_TITLE, dtype='str', delimiter=',', comments='!')\n",
    "y_c = np.loadtxt(INPUT_Y_C, delimiter=',', dtype=float)\n",
    "print('X:', X_C.shape, '   y:', y_c.shape)\n",
    "X_P = np.loadtxt(INPUT_X_P, delimiter=',')\n",
    "y_pp = np.loadtxt(INPUT_Y_P, delimiter=',', dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拉丁超立方采样\n",
    "data = []\n",
    "for _ in range(NUM_BATCH):\n",
    "    m = np.zeros((NUM_GENERATE, 4))\n",
    "    for i in range(3):\n",
    "        index = np.random.choice(NUM_GENERATE, NUM_GENERATE, replace=False)\n",
    "        inter = (C_LIMIT[i][1] - C_LIMIT[i][0])/NUM_GENERATE\n",
    "        for j in range(NUM_GENERATE):\n",
    "            m[j, i] = np.random.rand()*inter + C_LIMIT[i][0] + index[j]*inter\n",
    "    for i in range(m.shape[0]):\n",
    "        ttt = 1-m[i,0]/C0[0]-m[i,1]/C0[1]-m[i,2]/C0[2]\n",
    "        if ttt >= 0:\n",
    "            c4_ttt = C0[3]*ttt\n",
    "            temp = [m[i, 0], m[i, 1], m[i, 2], c4_ttt, m[i, 0]*C5_PRODUCT_TIME]\n",
    "            data.append(temp)\n",
    "print(len(data))\n",
    "X_p_C = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测是否还原\n",
    "clf = SVC(kernel='rbf', gamma=np.exp(-5.315672748087822), C=15.651302789809343, verbose=1, max_iter=-1, cache_size=40960, probability=True)\n",
    "paras = clf.get_params()\n",
    "point = round(X_C.shape[0]*TRAIN_TEST_SPLIT_1)\n",
    "y_p_m = []\n",
    "y_p_c = np.zeros((X_p_C.shape[0], 1))\n",
    "acc_list = []\n",
    "for _ in range(EPOCH):\n",
    "    permutation = np.random.permutation(y_c.shape[0])\n",
    "    train_idx = permutation[:point]\n",
    "    test_idx = permutation[point:]\n",
    "    X_train = X_C[train_idx, :]\n",
    "    y_train = y_c[train_idx]\n",
    "    X_test = X_C[test_idx, :]\n",
    "    y_test = y_c[test_idx]\n",
    "    clf_new = SVC()\n",
    "    for k, v in paras.items():\n",
    "        clf_new.set_params(**{k: v})\n",
    "    # 拟合模型\n",
    "    clf_new.fit(X_train, y_train)\n",
    "    # 计算损失\n",
    "    y_pred = clf_new.predict(X_test)\n",
    "    acc_count = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if y_pred[i]==y_test[i]:\n",
    "            acc_count += 1\n",
    "    acc = acc_count*100/X_test.shape[0]\n",
    "    y_p = clf_new.predict_proba(X_p_C)\n",
    "    y_p_l = y_p[:, 1].flatten().tolist()\n",
    "    acc_list.append(acc)\n",
    "    y_p_m.append(y_p_l)\n",
    "y_p_m = np.array(y_p_m)\n",
    "y_p_m = y_p_m.transpose()\n",
    "for i in range(X_p_C.shape[0]):\n",
    "    y_p_c[i, 0] = np.mean(y_p_m[i, :])\n",
    "del y_p_m\n",
    "data_nnn = []\n",
    "for i in range(y_p_c.shape[0]):\n",
    "    if y_p_c[i, 0] > RED_STANDARD:\n",
    "        data_nnn.append(X_p_C[i, :].flatten().tolist())\n",
    "print(len(data_nnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测是否生成黄色荧光\n",
    "X_p_t1 = np.array(data_nnn)\n",
    "clf = SVC(kernel='rbf', gamma=np.exp(-6.964045492214664), C=13.589042049663163, verbose=1, max_iter=-1, cache_size=40960, probability=True)\n",
    "paras = clf.get_params()\n",
    "point = round(X_P.shape[0]*TRAIN_TEST_SPLIT_2)\n",
    "y_p_m = []\n",
    "y_p_t1 = np.zeros((X_p_t1.shape[0], 1))\n",
    "acc_list = []\n",
    "for _ in range(EPOCH):\n",
    "    permutation1 = np.random.permutation(y_pp.shape[0])\n",
    "    train_idx = permutation1[:point]\n",
    "    test_idx = permutation1[point:]\n",
    "    X_train = X_P[train_idx, :]\n",
    "    y_train = y_pp[train_idx]\n",
    "    X_test = X_P[test_idx, :]\n",
    "    y_test = y_pp[test_idx]\n",
    "    clf_new = SVC()\n",
    "    for k, v in paras.items():\n",
    "        clf_new.set_params(**{k: v})\n",
    "    # 拟合模型\n",
    "    clf_new.fit(X_train, y_train)\n",
    "    # 计算损失\n",
    "    y_pred = clf_new.predict(X_test)\n",
    "    acc_count = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        if y_pred[i]==y_test[i]:\n",
    "            acc_count += 1\n",
    "    acc = acc_count*100/X_test.shape[0]\n",
    "    y_p = clf_new.predict_proba(X_p_t1)\n",
    "    y_p_l = y_p[:, 1].flatten().tolist()\n",
    "    acc_list.append(acc)\n",
    "    y_p_m.append(y_p_l)\n",
    "y_p_m = np.array(y_p_m)\n",
    "y_p_m = y_p_m.transpose()\n",
    "for i in range(X_p_t1.shape[0]):\n",
    "    y_p_t1[i, 0] = np.mean(y_p_m[i, :])\n",
    "del y_p_m\n",
    "data_next = []\n",
    "for i in range(y_p_t1.shape[0]):\n",
    "    if y_p_t1[i, 0] > YF_STANDARD:\n",
    "        data_next.append(X_p_t1[i, :].flatten().tolist())\n",
    "print(len(data_next))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_next = np.array(data_next)\n",
    "# 二次归一化\n",
    "data_nnn = data_next.copy()\n",
    "dd = data_nnn.copy()\n",
    "for i in range(dd.shape[1]):\n",
    "    dd[:, i] = (dd[:, i]-min(dd[:, i]))/(max(dd[:, i])-min(dd[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于欧几里得距离进行最远点采样\n",
    "final_idx = []\n",
    "d_m_2 = np.zeros((data_nnn.shape[0], data_nnn.shape[0]))\n",
    "for i in range(data_nnn.shape[0]):\n",
    "    for j in range(data_nnn.shape[0]):\n",
    "        if i!=j:\n",
    "            d_m_2[i, j] = np.sqrt(np.sum((dd[i, :] - dd[j, :])**2))\n",
    "final_idx.append(np.random.randint(data_nnn.shape[0]))\n",
    "for i in range(NUM_FINAL-1):\n",
    "    max_dis_dataset = 0\n",
    "    new_id = 0\n",
    "    for j in range(data_nnn.shape[0]):\n",
    "        if j not in final_idx:  # 从剩余数据集中搜索\n",
    "            min_dis = np.min(d_m_2[j, final_idx])  # 到点集的最小距离\n",
    "            if min_dis > max_dis_dataset:\n",
    "                max_dis_dataset = min_dis  # 更新剩余数据集中的最大的距离点集的最小距离\n",
    "                new_id = j\n",
    "    final_idx.append(new_id)\n",
    "final_data = data_nnn[final_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_data = []\n",
    "for i in range(final_data.shape[0]):\n",
    "    temp = []\n",
    "    for j in range(4):\n",
    "        temp.append(final_data[i, j]*V_TOTAL/C0[j])\n",
    "    V_data.append(temp)\n",
    "V_data = np.array(V_data)\n",
    "print(np.sum(V_data[:, 0]), np.sum(V_data[:, 1]), np.sum(V_data[:, 2]), np.sum(V_data[:, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出最终数据集\n",
    "save_name = 'ExpData_'+time.strftime(\"%Y%m%d\", time.localtime())+'_ML-Exp-1.csv'\n",
    "save_name = Path('.', DIR, save_name)\n",
    "np.savetxt(save_name, V_data[:36, :], fmt='%.3f', delimiter=',')\n",
    "save_name = 'ExpData_'+time.strftime(\"%Y%m%d\", time.localtime())+'_ML-Exp-2.csv'\n",
    "save_name = Path('.', DIR, save_name)\n",
    "np.savetxt(save_name, V_data[36:, :], fmt='%.3f', delimiter=',')\n",
    "save_name = 'ExpData_'+time.strftime(\"%Y%m%d\", time.localtime())+'_Exp.txt'\n",
    "save_name = Path('.', DIR, save_name)\n",
    "f1 = open(save_name, 'w')\n",
    "f1.write('Reagent 1: ZrT(0.5ML, 0.375+H2O)   C:'+str(round(C0[0], 2))+'mM   V:'+str(round(np.sum(V_data[:, 0]), 3))+'mL\\n')\n",
    "f1.write('Reagent 2: HAuCl4(2.00mL-->30mL)   C:'+str(round(C0[1], 2))+'mM   V:'+str(round(np.sum(V_data[:, 1]), 3))+'mL\\n')\n",
    "f1.write('Reagent 3: NEt3   C:'+str(round(C0[2], 2))+'mM   V:'+str(round(np.sum(V_data[:, 2]), 3))+'mL\\n')\n",
    "f1.write('Reagent 4: VC   C:'+str(round(C0[3], 2))+'mM   V:'+str(round(np.sum(V_data[:, 3]), 3))+'mL\\n')\n",
    "f1.write('Oven: 80.0℃    10.0hour(s)\\n')\n",
    "f1.close()\n",
    "id_list = []\n",
    "for i in range(72):\n",
    "    id_list.append(i+1)\n",
    "id_list = np.array(id_list).reshape(72, 1)\n",
    "out = np.hstack((id_list, V_data))\n",
    "out = np.hstack((out, final_data))\n",
    "out = np.hstack((out, np.zeros((final_data.shape[0], 1))))\n",
    "title = np.array(['Index', 'ZrT(mL)', 'HAuCl4(mL)', 'NEt3(mL)', 'VC(mL)', 'ZrT(mM)', 'HAuCl4(mM)', 'NEt3(mM)', 'VC(mM)', 'H2O(M)', 'Reduction']).reshape(1, 11)\n",
    "out = np.vstack((title, out))\n",
    "save_name = 'ExpData_'+time.strftime(\"%Y%m%d\", time.localtime())+'_Exp-Analysis.csv'\n",
    "save_name = Path('.', DIR, save_name)\n",
    "np.savetxt(save_name, out, fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonForDYH",
   "language": "python",
   "name": "dyhpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
